{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LESSON_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VorkovN/Computer-Vision-Course/blob/main/LESSON_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrGQvDKsJVgK"
      },
      "source": [
        "В данном блокноте мы рассмотрим несколько вариантов отслеживания объектов на видео.\n",
        "\n",
        "Начнем с метода Optical Flow, который, по своей сути, сводится к трекингу всех пикселей на видео. Обычно этот метод визуализируют либо непосредственно векторами, либо цветом (каждый цвет кодирует определенный вектор).\n",
        "\n",
        "Для начала загрузим необходимые библиотеки.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti-Ms8OfJyFv"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd91njdGznUP"
      },
      "source": [
        "Теперь напишем функцию, которая будет полезна для визуализации векторами. На основании полученных данных, она на исходном кадре будет рисовать вектор (наше видео будет обрабатываться покадрово)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgTJSBYKM9V"
      },
      "source": [
        "def draw_flow(img, flow):\n",
        "  step=30\n",
        "  h, w = img.shape[:2]\n",
        "  y, x = np.mgrid[step / 2:h:step, step / 2:w:step].reshape(2, -1).astype(int)\n",
        "  fx, fy = flow[y, x].T\n",
        "  lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\n",
        "  lines = np.int32(lines + 0.5)\n",
        "  image = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "  for (x1, y1), (x2, y2) in lines:\n",
        "    cv2.arrowedLine(image, (x1, y1), (x2, y2), (255, 0, 0), 1, cv2.LINE_AA, 0, 0.3)\n",
        "  return image"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOzQW3Uf0Kir"
      },
      "source": [
        "Считаем наш видео файл средствами библиотеки OpenCV. Данный метод предполагает работу с черно-белыми изображениями. Сразу же создадим копию первого кадра нужного размера и закрасим её черным (именно на ней мы увидим визуализацию цветом)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uycmXvVu1QLL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "3f4ff771-b239-49d0-e637-c453777986ef"
      },
      "source": [
        "cap = cv2.VideoCapture(\"Airport _f.mp4\")\n",
        "ret, frame1 = cap.read()\n",
        "prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
        "hsv = np.zeros_like(frame1)\n",
        "hsv[...,1] = 255"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-44a786f437de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Airport _f.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhsv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-XlKhCO1QpZ"
      },
      "source": [
        "К сожалению, данная среда разработки не позволяет демонстрировать видео-файлы. Поэтому мы будем покадрово записывать наши результаты в файл, который в дальнейшем можно посмотреть на любом плеере."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLHYUhzm12bv"
      },
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "videoWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "videoHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "out = cv2.VideoWriter('output_OpticalFlow.avi',fourcc, 20.0,(videoWidth,videoHeight))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqTFMrcvKtYw"
      },
      "source": [
        "kk = 0\n",
        "while(cap.isOpened()):\n",
        "    # Считываем видео кадр за кадром\n",
        "    ret, frame2 = cap.read() \n",
        "    if ret:\n",
        "      next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
        "      flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "      # Получаем векторы и заполняем значениями нашу копию для визуализации цветом\n",
        "      mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
        "      hsv[...,0] = ang*180/np.pi/2\n",
        "      hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
        "      bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
        "      # Выведем первые 10 кадров на экран с нарисованными векторами\n",
        "      if kk < 10:\n",
        "        cv2_imshow(draw_flow(next, flow))\n",
        "      # Записываем кадр в результирующий файл\n",
        "      out.write(bgr)\n",
        "      # Переобозначаем кадры - предыдущий становится текущим\n",
        "      prvs = next\n",
        "      kk += 1\n",
        "    else:\n",
        "      break\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JsWvW1D8e1P"
      },
      "source": [
        "Теперь посмотрим как работает на нашем видео другой детектор — SORT (Simple Online and Realtime Tracking). Для сопоставления траекторий и обнаружения новых объектов, мы будем использовать алгоритм YOLO (You Only Look Once) - венгерский алгоритм, использующий фильтр Калмана для предсказания и корректировки сегментов.\n",
        "\n",
        "Для начала доустановим необходимые библиотеки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Ath_5j8fy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecacfb21-2bb2-4c3b-87b1-d77f5ec2fdea"
      },
      "source": [
        "!pip install filterpy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from filterpy) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from filterpy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->filterpy) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy) (1.15.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=ad050199ebe17441dc9beef8998dff6f6b52d7edb088f0485a7ef96399886e7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jab6-Ggj8m-g"
      },
      "source": [
        "import random\n",
        "\n",
        "try:\n",
        "  from numba import jit\n",
        "except:\n",
        "  def jit(func):\n",
        "    return func\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "from filterpy.kalman import KalmanFilter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp63qp6v8wl7"
      },
      "source": [
        "Воспользуемся открытой для использования уже натренированной моделью Darknet (конфигурация, веса) на СОСО данных. Оставляем все комментарии в исходном виде."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az13Xtyu8zJG"
      },
      "source": [
        "def linear_assignment(cost_matrix):\n",
        "  try:\n",
        "    import lap\n",
        "    _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n",
        "    return np.array([[y[i],i] for i in x if i >= 0]) #\n",
        "  except ImportError:\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "    x, y = linear_sum_assignment(cost_matrix)\n",
        "    return np.array(list(zip(x, y)))\n",
        "\n",
        "\n",
        "@jit\n",
        "def iou(bb_test, bb_gt):\n",
        "  \"\"\"\n",
        "  Computes IUO between two bboxes in the form [x1,y1,x2,y2]\n",
        "  \"\"\"\n",
        "  xx1 = np.maximum(bb_test[0], bb_gt[0])\n",
        "  yy1 = np.maximum(bb_test[1], bb_gt[1])\n",
        "  xx2 = np.minimum(bb_test[2], bb_gt[2])\n",
        "  yy2 = np.minimum(bb_test[3], bb_gt[3])\n",
        "  w = np.maximum(0., xx2 - xx1)\n",
        "  h = np.maximum(0., yy2 - yy1)\n",
        "  wh = w * h\n",
        "  o = wh / ((bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1])\n",
        "    + (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1]) - wh)\n",
        "  return(o)\n",
        "\n",
        "\n",
        "def convert_bbox_to_z(bbox):\n",
        "  \"\"\"\n",
        "  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n",
        "    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n",
        "    the aspect ratio\n",
        "  \"\"\"\n",
        "  w = bbox[2] - bbox[0]\n",
        "  h = bbox[3] - bbox[1]\n",
        "  x = bbox[0] + w/2.\n",
        "  y = bbox[1] + h/2.\n",
        "  s = w * h    #scale is just area\n",
        "  r = w / float(h)\n",
        "  return np.array([x, y, s, r]).reshape((4, 1))\n",
        "\n",
        "\n",
        "def convert_x_to_bbox(x,score=None):\n",
        "  \"\"\"\n",
        "  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n",
        "    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n",
        "  \"\"\"\n",
        "  w = np.sqrt(x[2] * x[3])\n",
        "  h = x[2] / w\n",
        "  if(score==None):\n",
        "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n",
        "  else:\n",
        "    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\n",
        "\n",
        "\n",
        "class KalmanBoxTracker(object):\n",
        "  \"\"\"\n",
        "  This class represents the internal state of individual tracked objects observed as bbox.\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  def __init__(self,bbox):\n",
        "    \"\"\"\n",
        "    Initialises a tracker using initial bounding box.\n",
        "    \"\"\"\n",
        "    #define constant velocity model\n",
        "    self.kf = KalmanFilter(dim_x=7, dim_z=4) \n",
        "    self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n",
        "    self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n",
        "\n",
        "    self.kf.R[2:,2:] *= 10.\n",
        "    self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n",
        "    self.kf.P *= 10.\n",
        "    self.kf.Q[-1,-1] *= 0.01\n",
        "    self.kf.Q[4:,4:] *= 0.01\n",
        "\n",
        "    self.kf.x[:4] = convert_bbox_to_z(bbox)\n",
        "    self.time_since_update = 0\n",
        "    self.id = KalmanBoxTracker.count\n",
        "    KalmanBoxTracker.count += 1\n",
        "    self.history = []\n",
        "    self.hits = 0\n",
        "    self.hit_streak = 0\n",
        "    self.age = 0\n",
        "\n",
        "  def update(self,bbox):\n",
        "    \"\"\"\n",
        "    Updates the state vector with observed bbox.\n",
        "    \"\"\"\n",
        "    self.time_since_update = 0\n",
        "    self.history = []\n",
        "    self.hits += 1\n",
        "    self.hit_streak += 1\n",
        "    self.kf.update(convert_bbox_to_z(bbox))\n",
        "\n",
        "  def predict(self):\n",
        "    \"\"\"\n",
        "    Advances the state vector and returns the predicted bounding box estimate.\n",
        "    \"\"\"\n",
        "    if((self.kf.x[6]+self.kf.x[2])<=0):\n",
        "      self.kf.x[6] *= 0.0\n",
        "    self.kf.predict()\n",
        "    self.age += 1\n",
        "    if(self.time_since_update>0):\n",
        "      self.hit_streak = 0\n",
        "    self.time_since_update += 1\n",
        "    self.history.append(convert_x_to_bbox(self.kf.x))\n",
        "    return self.history[-1]\n",
        "\n",
        "  def get_state(self):\n",
        "    \"\"\"\n",
        "    Returns the current bounding box estimate.\n",
        "    \"\"\"\n",
        "    return convert_x_to_bbox(self.kf.x)\n",
        "\n",
        "\n",
        "def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):\n",
        "  \"\"\"\n",
        "  Assigns detections to tracked object (both represented as bounding boxes)\n",
        "  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n",
        "  \"\"\"\n",
        "  if(len(trackers)==0):\n",
        "    return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n",
        "  iou_matrix = np.zeros((len(detections),len(trackers)),dtype=np.float32)\n",
        "\n",
        "  for d,det in enumerate(detections):\n",
        "    for t,trk in enumerate(trackers):\n",
        "      iou_matrix[d,t] = iou(det,trk)\n",
        "\n",
        "  if min(iou_matrix.shape) > 0:\n",
        "    a = (iou_matrix > iou_threshold).astype(np.int32)\n",
        "    if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n",
        "        matched_indices = np.stack(np.where(a), axis=1)\n",
        "    else:\n",
        "      matched_indices = linear_assignment(-iou_matrix)\n",
        "  else:\n",
        "    matched_indices = np.empty(shape=(0,2))\n",
        "\n",
        "  unmatched_detections = []\n",
        "  for d, det in enumerate(detections):\n",
        "    if(d not in matched_indices[:,0]):\n",
        "      unmatched_detections.append(d)\n",
        "  unmatched_trackers = []\n",
        "  for t, trk in enumerate(trackers):\n",
        "    if(t not in matched_indices[:,1]):\n",
        "      unmatched_trackers.append(t)\n",
        "\n",
        "  #filter out matched with low IOU\n",
        "  matches = []\n",
        "  for m in matched_indices:\n",
        "    if(iou_matrix[m[0], m[1]]<iou_threshold):\n",
        "      unmatched_detections.append(m[0])\n",
        "      unmatched_trackers.append(m[1])\n",
        "    else:\n",
        "      matches.append(m.reshape(1,2))\n",
        "  if(len(matches)==0):\n",
        "    matches = np.empty((0,2),dtype=int)\n",
        "  else:\n",
        "    matches = np.concatenate(matches,axis=0)\n",
        "\n",
        "  return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n",
        "\n",
        "\n",
        "class Sort(object):\n",
        "  def __init__(self, max_age=1, min_hits=3):\n",
        "    \"\"\"\n",
        "    Sets key parameters for SORT\n",
        "    \"\"\"\n",
        "    self.max_age = max_age\n",
        "    self.min_hits = min_hits\n",
        "    self.trackers = []\n",
        "    self.frame_count = 0\n",
        "\n",
        "  def update(self, dets=np.empty((0, 5))):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n",
        "    Requires: this method must be called once for each frame even with empty detections (use np.empty((0, 5)) for frames without detections).\n",
        "    Returns the a similar array, where the last column is the object ID.\n",
        "    NOTE: The number of objects returned may differ from the number of detections provided.\n",
        "    \"\"\"\n",
        "    self.frame_count += 1\n",
        "    # get predicted locations from existing trackers.\n",
        "    trks = np.zeros((len(self.trackers), 5))\n",
        "    to_del = []\n",
        "    ret = []\n",
        "    for t, trk in enumerate(trks):\n",
        "      pos = self.trackers[t].predict()[0]\n",
        "      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n",
        "      if np.any(np.isnan(pos)):\n",
        "        to_del.append(t)\n",
        "    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n",
        "    for t in reversed(to_del):\n",
        "      self.trackers.pop(t)\n",
        "    matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets,trks)\n",
        "\n",
        "    # update matched trackers with assigned detections\n",
        "    for m in matched:\n",
        "      self.trackers[m[1]].update(dets[m[0], :])\n",
        "\n",
        "    # create and initialise new trackers for unmatched detections\n",
        "    for i in unmatched_dets:\n",
        "        trk = KalmanBoxTracker(dets[i,:])\n",
        "        self.trackers.append(trk)\n",
        "    i = len(self.trackers)\n",
        "    for trk in reversed(self.trackers):\n",
        "        d = trk.get_state()[0]\n",
        "        if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n",
        "          ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n",
        "        i -= 1\n",
        "        # remove dead tracklet\n",
        "        if(trk.time_since_update > self.max_age):\n",
        "          self.trackers.pop(i)\n",
        "    if(len(ret)>0):\n",
        "      return np.concatenate(ret)\n",
        "    return np.empty((0,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0veXDOfJ838q"
      },
      "source": [
        "Необходимые веса yolov3.weights можно скачать с сайта https://pjreddie.com/darknet/yolo/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2E1DU7-86u8"
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "if not exists('yolov3.weights'):\n",
        "  !wget -q  https://pjreddie.com/media/files/yolov3.weights  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg9vK5VD89dk"
      },
      "source": [
        "А теперь перейдем непосредственно к детектированию нашего видео. Опять-таки из-за проблем с представлением видео-файлов, мы будем писать результат в отдельный файл."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu9YXUIb9DT-"
      },
      "source": [
        "def load_class_names(namesfile):\n",
        "    class_names = []\n",
        "    with open(namesfile, 'r') as fp:\n",
        "        lines = fp.readlines()\n",
        "    for line in lines:\n",
        "        line = line.rstrip()\n",
        "        class_names.append(line)\n",
        "    return class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buX28j4u9JV0"
      },
      "source": [
        "# Загружаем сеть\n",
        "net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
        "ln = net.getLayerNames()\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "# Инициализируем трекер\n",
        "mot_tracker = Sort()\n",
        "# Используем YOLO\n",
        "namesfile = 'coco.names'\n",
        "class_names = load_class_names(namesfile)\n",
        "# Указываем необходимый видео-файл\n",
        "cap = cv2.VideoCapture(\"Airport _f.mp4\")\n",
        "# Указываем кодеки для результирующего видео\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "videoWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "videoHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# Определяем результирующее видео\n",
        "out = cv2.VideoWriter('output_SORT.avi',fourcc, 20.0,(videoWidth,videoHeight))\n",
        "# Генерируем будующим прямоугольникам разные цвета\n",
        "color_list = []\n",
        "for j in range(1000):\n",
        "  color_list.append(((int)(random.randrange(255)),(int)(random.randrange(255)),(int)(random.randrange(255))))\n",
        "\n",
        "kk = 0 \n",
        "ret = True\n",
        "while ret:\n",
        "  ret, img = cap.read()\n",
        "  if ret:\n",
        "  # Запустим сеть по кадру\n",
        "    blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    h,w,_ = img.shape\n",
        "    net.setInput(blob)\n",
        "    layerOutputs = net.forward(ln)\n",
        "    # Разберём все выходы\n",
        "    boxes=[]\n",
        "    confidences=[]\n",
        "    classIDs=[]\n",
        "    for output in layerOutputs:\n",
        "      for detection in output:\n",
        "        scores = detection[5:]\n",
        "        classID = np.argmax(scores)\n",
        "        confidence = scores[classID]\n",
        "        if confidence > 0.5:\n",
        "          box = detection[0:4] * np.array([w, h, w, h])\n",
        "          (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "          x = int(centerX - (width / 2))\n",
        "          y = int(centerY - (height / 2))\n",
        "          boxes.append([x, y, int(width), int(height)])\n",
        "          confidences.append(float(confidence))\n",
        "          classIDs.append(classID)\n",
        "    else: \n",
        "      break\n",
        "      \n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
        "    result_img = np.copy(img)\n",
        "    dets = []\n",
        "    count_detection=0\n",
        "    for j in range(len(idxs)):\n",
        "      name = class_names[classIDs[idxs[j][0]]]\n",
        "      if name == 'aeroplane':\n",
        "        count_detection+=1\n",
        "    if count_detection>0:\n",
        "      detects = np.zeros((count_detection,5))\n",
        "      count=0\n",
        "      # Формат, необходимый для трекера\n",
        "      for j in range(len(idxs)):\n",
        "        b = boxes[idxs[j][0]]\n",
        "        name = class_names[classIDs[idxs[j][0]]]\n",
        "        if name == 'aeroplane': # указываем необходимую метку для объектов\n",
        "          x1 = int(b[0])\n",
        "          y1 = int(b[1])\n",
        "          x2 = int((b[0] + b[2]))\n",
        "          y2 = int((b[1] + b[3]))\n",
        "          box = np.array([x1,y1,x2,y2,confidences[idxs[j][0]]])\n",
        "          detects[count,:] = box[:]\n",
        "          count+=1\n",
        "      # Парсим данные трекера\n",
        "      if len(detects)!=0:\n",
        "        trackers = mot_tracker.update(detects)\n",
        "        for d in trackers:\n",
        "          result_img = cv2.rectangle(result_img, ((int)(d[0]), (int)(d[1])), ((int)(d[2]), (int)(d[3])), color_list[(int)(d[4])], 2)\n",
        "          result_img = cv2.putText(result_img, name + \"-\" + str(int(d[4])), ((int)(d[0]), (int)(d[1]) - 10), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, color_list[(int)(d[4])], 2)\n",
        "    \n",
        "    if kk % 20 == 0 and kk < 100:\n",
        "      cv2_imshow(result_img) # выводим только ограниченное количество кадров \n",
        "    out.write(result_img) # пишем кадр в результирующее видео\n",
        "    kk += 1\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}